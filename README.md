1: Create custom-model configuration file as given (custom-model)

2: Configure Custom model (LLama3) using Ollama with following command
ollama create llama3-medicalchatbot -f custom-model

3: run llama3 on your local machine using following command
ollama run llama3-medicalchatbot:latest

4: connect with streamlit
git clone https://github.com/tonykipkemboi/ollama_streamlit_demos.git

5:cd ollama_streamlit_demos

6:pip install -r requirements.txt

7: streamlit run 01_ðŸ’¬_Chat.py (after nevigating to dir containing /Built_Llama_With_OLlama(Final))
